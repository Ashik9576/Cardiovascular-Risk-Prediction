{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NETFLIX MOVIES AND TV SHOWS CLUSTERING.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ashik9576/Carduovascular-Risk-Prediction/blob/main/NETFLIX_MOVIES_AND_TV_SHOWS_CLUSTERING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGcl5Bv9ed6u"
      },
      "source": [
        "# **Problem Statement**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M5kyFaVjzXk"
      },
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming serviceâ€™s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLx5cJO8hzb1"
      },
      "source": [
        "## <b>In this  project, you are required to do </b>\n",
        "1. Exploratory Data Analysis \n",
        "\n",
        "2. Understanding what type content is available in different countries\n",
        "\n",
        "3. Is Netflix has increasingly focusing on TV rather than movies in recent years.\n",
        "4. Clustering similar content by matching text-based features\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzGDqdC4fZ-b"
      },
      "source": [
        "# **Attribute Information**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhfLYargscGV"
      },
      "source": [
        "1. show_id : Unique ID for every Movie / Tv Show\n",
        "\n",
        "2. type : Identifier - A Movie or TV Show\n",
        "\n",
        "3. title : Title of the Movie / Tv Show\n",
        "\n",
        "4. director : Director of the Movie\n",
        "\n",
        "5. cast : Actors involved in the movie / show\n",
        "\n",
        "6. country : Country where the movie / show was produced\n",
        "\n",
        "7. date_added : Date it was added on Netflix\n",
        "\n",
        "8. release_year : Actual Releaseyear of the movie / show\n",
        "\n",
        "9. rating : TV Rating of the movie / show\n",
        "\n",
        "10. duration : Total Duration - in minutes or number of seasons\n",
        "\n",
        "11. listed_in : Genere\n",
        "\n",
        "12. description: The Summary description"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Libraries**"
      ],
      "metadata": {
        "id": "ivfYJUlGsOso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from time import time\n",
        "import keras.backend as K\n",
        "from tensorflow.python.keras.layers import Layer, InputSpec\n",
        "#from tensorflow.keras.engine.topology import Layer, InputSpec\n",
        "from keras.layers import Dense, Input, Embedding\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import callbacks\n",
        "from keras.initializers import VarianceScaling\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "CRFUfV0CsXOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mounted the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "48Om_utfsaPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading DataSet**"
      ],
      "metadata": {
        "id": "6o-IiZGesgz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating directory path\n",
        "df = pd.read_csv('/content/drive/MyDrive/NETFLIX MOVIES AND TV SHOWS CLUSTERING/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "df_netflix=df.copy()"
      ],
      "metadata": {
        "id": "_PCTxHPnsoRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **File Structure and content Information**"
      ],
      "metadata": {
        "id": "FeYCgEK9tDAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#top 10 rows of the dataset\n",
        "df.head()"
      ],
      "metadata": {
        "id": "fzqzQrQmtIYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shape of the dataset\n",
        "shape=df.shape\n",
        "print(\"Rows :\"+str(shape[0]) +\", Columns :\"+str(shape[1]))"
      ],
      "metadata": {
        "id": "jS0FY8n2tMbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#information of the dataset\n",
        "df.info()"
      ],
      "metadata": {
        "id": "5Bz7NZnqth6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Missing Value Treatment**\n",
        "Knowing about missing values is important because they indicate how much we dont's know about our data. Making inferences based on just a few cases is often unwise. In addition, many modelling procedures break down when missing values are involved and the corresponding rows will either have to be removed completely or the values need to be estimated somehow."
      ],
      "metadata": {
        "id": "bOaR3YQbwIkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check null values in Data frame\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "jqcDba5CwSjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lqXL_-NMxtgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Describing dataset**"
      ],
      "metadata": {
        "id": "D0zOexN8xtjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# describing the dataset\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "IDBiEoO-uQkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check count of unique id's in the dataset\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "sM9Wk-49ubNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B5Jyp3KBx_10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reformatting Features**"
      ],
      "metadata": {
        "id": "HCf1d0aUyOOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lCdmui4byU25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **UNIVARIATE ANALYSIS**"
      ],
      "metadata": {
        "id": "wC3F6n5xN1oq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='type',data=df)"
      ],
      "metadata": {
        "id": "hpHezVX6N9CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "#Define displayed years:\n",
        "years = list(range(2008,2020,1))\n",
        "\n",
        "#separate movies and tv_shows:\n",
        "movie_rows = df_netflix.loc[df_netflix[\"type\"] == \"Movie\"]\n",
        "tv_rows = df_netflix.loc[df_netflix[\"type\"] == \"TV Show\"]\n",
        "\n",
        "#Count movies / tv shows per year\n",
        "movies_counts = movie_rows.release_year.value_counts()\n",
        "tv_counts = tv_rows.release_year.value_counts()\n",
        "\n",
        "index_years_mov = movies_counts.index.isin(years)\n",
        "index_years_tv = tv_counts.index.isin(years)\n",
        "\n",
        "#select movies / tv shows between chosen years:\n",
        "movies = movies_counts[index_years_mov]\n",
        "tv_shows = tv_counts[index_years_tv]\n",
        "\n",
        "# Calculate percentages of movies and tv shows:\n",
        "movies_per = round(movie_rows.shape[0] / df_netflix[\"type\"].shape[0] * 100, 2)\n",
        "tvshows_per = round(tv_rows.shape[0] / df_netflix[\"type\"].shape[0] * 100, 2)\n",
        "\n",
        "#Top Movie and TV Show producer country:\n",
        "top5_producer_countrys = df_netflix.country.value_counts().sort_values(ascending=False).head(5)\n",
        "\n",
        "#Top most commen Actors an directors (Movies and tv shows):\n",
        "casts = \", \".join(df_netflix.copy().fillna(\"\")['cast']).split(\", \")\n",
        "counter_list = Counter(casts).most_common(5)\n",
        "most_commen_actors = [i for i in counter_list if i[0] != \"\"]\n",
        "labels = [i[0] for i in most_commen_actors][::-1]\n",
        "values = [i[1] for i in most_commen_actors][::-1]\n",
        "\n",
        "most_commen_directors = df_netflix.director.value_counts().head(5).sort_values(ascending=True)"
      ],
      "metadata": {
        "id": "SIsxuUnTOFDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "width = 0.75\n",
        "\n",
        "sns.set(style=\"whitegrid\", palette=\"muted\", color_codes=True)\n",
        "\n",
        "def autolabel(rects, axes):\n",
        "    \"\"\"Helper function to attach a text label above each bar in *rects*, displaying its height.\n",
        "        Add specific axes[x, y] for subplot labeling\"\"\"\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        axes.annotate('{}'.format(height),\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),  # 3 points vertical offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "f, axes = plt.subplots(2, 2, figsize=(12, 12), sharex=False)\n",
        "\n",
        "#Line plot of Movies and TV Shows released by Netflix per year\")\n",
        "sns.lineplot(data=movies, color=\"b\", ax=axes[0, 0], label=\"Movies / year\")\n",
        "sns.lineplot(data=tv_shows, color=\"c\", ax=axes[0, 0], label=\"TV Shows / year\")\n",
        "\n",
        "# Pie chart of type percentages\n",
        "axes[0, 1].pie([movies_per, tvshows_per], explode=(0, 0.1,), labels=[\"Movies\", \"TV Shows\"], autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90)\n",
        "\n",
        "# Bar chart of top 5 Movie / Tv shows producer countrys:\n",
        "rects1 = axes[1, 0].bar(top5_producer_countrys.index, top5_producer_countrys.values,)\n",
        "\n",
        "autolabel(rects1, axes[1, 0])\n",
        "\n",
        "#Bar chart of top 5 most commen actors and directors:\n",
        "rects2 = axes[1, 1].bar(labels, values, width, label='Actors',)\n",
        "\n",
        "rects3 = axes[1, 1].bar(most_commen_directors.index, most_commen_directors.values, width, label='Directors')\n",
        "\n",
        "autolabel(rects2, axes[1, 1])\n",
        "autolabel(rects3, axes[1, 1])\n",
        "\n",
        "axes[0, 0].set_ylabel('Publications')\n",
        "axes[0, 0].set_title('Movies / Tv Shows relesed per year')\n",
        "\n",
        "axes[0, 1].set_title('Percentage of Movies and Tv Shows')\n",
        "\n",
        "axes[1, 0].set_ylabel('Movies and Tv Shows')\n",
        "axes[1, 0].set_title('Top 5 producer countrys')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "axes[1, 1].set_ylabel('Number Occurring')\n",
        "axes[1, 1].set_xticklabels(labels + list(most_commen_directors.index), rotation=\"vertical\")\n",
        "axes[1, 1].set_title('Top 5 most commen actors and directors')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('output.png')\n",
        "plt.show()\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(filename='output.png')"
      ],
      "metadata": {
        "id": "cZYDN_b-PM2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m_s_groups = df_netflix.groupby([\"title\", \"director\", \"listed_in\",]).apply(lambda df: df.title) #Returns Pandas Series with movie / series title and original index\n",
        "m_s_groups.head()"
      ],
      "metadata": {
        "id": "hZlMBSjaSVlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_token = df_netflix[ \"description\"]\n",
        "#df_token = df_netflix[[\"listed_in\", \"description\"]].values.tolist()\n",
        "\n",
        "maxlen = 1500 #only use this number of most frequent words\n",
        "training_samples = 800\n",
        "validation_samples = 450\n",
        "max_words = 10000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(df_token) # generates word index\n",
        "sequences = tokenizer.texts_to_sequences(df_token) # transforms strings in list of intergers\n",
        "word_index = tokenizer.word_index # calculated word index\n",
        "print(f\"{len(word_index)} unique tokens found\")\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen) #transforms integer lists into 2D tensor"
      ],
      "metadata": {
        "id": "O_Cb0DUnbiKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler() \n",
        "x = scaler.fit_transform(data) # the values of all features are rescaled into the range of [0, 1]"
      ],
      "metadata": {
        "id": "vCyvPkLBbyBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
        "    \"\"\"\n",
        "    Fully connected symmetric auto-encoder model.\n",
        "  \n",
        "    dims: list of the sizes of layers of encoder like [500, 500, 2000, 10]. \n",
        "          dims[0] is input dim, dims[-1] is size of the latent hidden layer.\n",
        "\n",
        "    act: activation function\n",
        "    \n",
        "    return:\n",
        "        (autoencoder_model, encoder_model): Model of autoencoder and model of encoder\n",
        "    \"\"\"\n",
        "    n_stacks = len(dims) - 1\n",
        "    \n",
        "    input_data = Input(shape=(dims[0],), name='input')\n",
        "    x = input_data\n",
        "    \n",
        "    # internal layers of encoder\n",
        "    for i in range(n_stacks-1):\n",
        "        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
        "\n",
        "    # latent hidden layer\n",
        "    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)\n",
        "\n",
        "    x = encoded\n",
        "    # internal layers of decoder\n",
        "    for i in range(n_stacks-1, 0, -1):\n",
        "        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
        "\n",
        "    # decoder output\n",
        "    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
        "    \n",
        "    decoded = x\n",
        "    \n",
        "    autoencoder_model = Model(inputs=input_data, outputs=decoded, name='autoencoder')\n",
        "    encoder_model     = Model(inputs=input_data, outputs=encoded, name='encoder')\n",
        "    \n",
        "    return autoencoder_model, encoder_model"
      ],
      "metadata": {
        "id": "njIx-HQadv9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_clusters = 20 # max numbers of clusters\n",
        "n_epochs   = 8 # epchos for autencoder training\n",
        "batch_size = 128"
      ],
      "metadata": {
        "id": "Ixt7Npb7eXNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dims = [x.shape[-1], 500, 500, 1000, 10] \n",
        "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
        "                           distribution='uniform')\n",
        "pretrain_optimizer = \"rmsprop\" #SGD(lr=1, momentum=0.9)\n",
        "pretrain_epochs = n_epochs\n",
        "batch_size = batch_size"
      ],
      "metadata": {
        "id": "8CE1xKGgeafb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dims"
      ],
      "metadata": {
        "id": "pBJ_cqxUedjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClusteringLayer(Layer):\n",
        "    '''\n",
        "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
        "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
        "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
        "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
        "        super(ClusteringLayer, self).__init__(**kwargs)\n",
        "        self.n_clusters = n_clusters\n",
        "        self.alpha = alpha\n",
        "        self.initial_weights = weights\n",
        "        self.input_spec = InputSpec(ndim=2)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 2\n",
        "        input_dim = input_shape[1]\n",
        "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
        "        self.clusters = self.add_weight(name='clusters', shape=(self.n_clusters, input_dim), initializer='glorot_uniform') \n",
        "        \n",
        "        if self.initial_weights is not None:\n",
        "            self.set_weights(self.initial_weights)\n",
        "            del self.initial_weights\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        ''' \n",
        "        student t-distribution, as used in t-SNE algorithm.\n",
        "        It measures the similarity between embedded point z_i and centroid Âµ_j.\n",
        "                 q_ij = 1/(1+dist(x_i, Âµ_j)^2), then normalize it.\n",
        "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
        "                 (i.e., a soft assignment)\n",
        "       \n",
        "        inputs: the variable containing data, shape=(n_samples, n_features)\n",
        "        \n",
        "        Return: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
        "        '''\n",
        "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
        "        q **= (self.alpha + 1.0) / 2.0\n",
        "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure all of the values of each sample sum up to 1.\n",
        "        \n",
        "        return q\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        assert input_shape and len(input_shape) == 2\n",
        "        return input_shape[0], self.n_clusters\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'n_clusters': self.n_clusters}\n",
        "        base_config = super(ClusteringLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "metadata": {
        "id": "gSQ5hbjjmrEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder, encoder = autoencoder(dims, init=init)"
      ],
      "metadata": {
        "id": "_afkjAq0paTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer=pretrain_optimizer, loss='binary_crossentropy')  #loss='mse'\n",
        "autoencoder.fit(x, x, batch_size=batch_size, epochs=pretrain_epochs)\n",
        "#autoencoder.save_weights(save_dir + '/ae_weights.h5')"
      ],
      "metadata": {
        "id": "bvsC6snfrKFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
        "model = Model(inputs=encoder.input, outputs=clustering_layer)"
      ],
      "metadata": {
        "id": "kbZ90m7-rRj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=SGD(0.01, 0.9), loss='kld') #(optimizer=SGD(0.01, 0.9), loss='kld')"
      ],
      "metadata": {
        "id": "NGx70WerrZUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters=n_clusters, n_init=20)\n",
        "y_pred = kmeans.fit_predict(encoder.predict(x))"
      ],
      "metadata": {
        "id": "wXgRwJMTrdjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_last = np.copy(y_pred)"
      ],
      "metadata": {
        "id": "qA6B7VwIrgzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])"
      ],
      "metadata": {
        "id": "1CW3FO0Hrk1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# computing an auxiliary target distribution\n",
        "def target_distribution(q):\n",
        "    weight = q ** 2 / q.sum(0)\n",
        "    return (weight.T / weight.sum(1)).T"
      ],
      "metadata": {
        "id": "wbRZJ5urrmsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = 0\n",
        "index = 0\n",
        "maxiter = 1000 # 8000\n",
        "update_interval = 100 # 140\n",
        "index_array = np.arange(x.shape[0])"
      ],
      "metadata": {
        "id": "T75of0P7romb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tol = 0.001 # tolerance threshold to stop training"
      ],
      "metadata": {
        "id": "JqAE2mfxrre5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ite in range(int(maxiter)):\n",
        "    if ite % update_interval == 0:\n",
        "        q = model.predict(x, verbose=0)\n",
        "        p = target_distribution(q)  # update the auxiliary target distribution p\n",
        "\n",
        "    idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
        "    loss = model.train_on_batch(x=x[idx], y=p[idx])\n",
        "    index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
        "\n",
        "#model.save_weights(save_dir + '/DEC_model_final.h5')"
      ],
      "metadata": {
        "id": "LkVHSHnRsNtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval.\n",
        "q = model.predict(x, verbose=0)\n",
        "p = target_distribution(q)  # update the auxiliary target distribution p\n",
        "\n",
        "# evaluate the clustering performance\n",
        "y_pred = q.argmax(1)"
      ],
      "metadata": {
        "id": "Ox-WPyPdsRol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_all = df_netflix.copy()"
      ],
      "metadata": {
        "id": "DclfG5uEsU63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_all['cluster'] = y_pred\n",
        "data_all.head()"
      ],
      "metadata": {
        "id": "FK6v_RxdsYN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_all['cluster'].value_counts()"
      ],
      "metadata": {
        "id": "_zkuF82vsdv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "x_embedded = TSNE(n_components=2).fit_transform(x)\n",
        "\n",
        "x_embedded.shape"
      ],
      "metadata": {
        "id": "Ur0ty5Lksj-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# sns settings\n",
        "sns.set(rc={'figure.figsize':(15,15)})\n",
        "\n",
        "# colors\n",
        "palette = sns.color_palette(\"bright\", len(set(y_pred)))\n",
        "\n",
        "# plot\n",
        "sns.scatterplot(x_embedded[:,0], x_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n",
        "plt.title(\"Netflix Movies and Tv Shows, Clustered(Autoencoder and custem Keras Layer), Tf-idf with Plain Text\")\n",
        "plt.savefig('output2.png')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(filename='output2.png')"
      ],
      "metadata": {
        "id": "JLxXBWTCsuuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS\n",
        "from bokeh.palettes import Category20\n",
        "from bokeh.transform import linear_cmap\n",
        "from bokeh.io import output_file, show\n",
        "from bokeh.transform import transform\n",
        "from bokeh.io import output_notebook\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.layouts import column\n",
        "from bokeh.models import RadioButtonGroup\n",
        "from bokeh.models import TextInput\n",
        "from bokeh.layouts import gridplot\n",
        "from bokeh.models import Div\n",
        "from bokeh.models import Paragraph\n",
        "from bokeh.layouts import column, widgetbox\n",
        "\n",
        "output_notebook()\n",
        "y_labels = y_pred\n",
        "\n",
        "# data sources\n",
        "source = ColumnDataSource(data=dict(\n",
        "    x= x_embedded[:,0], \n",
        "    y= x_embedded[:,1],\n",
        "    x_backup = x_embedded[:,0],\n",
        "    y_backup = x_embedded[:,1],\n",
        "    desc= y_labels, \n",
        "    titles= df_netflix['title'],\n",
        "    directors = df_netflix['director'],\n",
        "    cast = df_netflix['cast'],\n",
        "    description = df_netflix['description'],\n",
        "    labels = [\"C-\" + str(x) for x in y_labels]\n",
        "    ))\n",
        "\n",
        "# hover over information\n",
        "hover = HoverTool(tooltips=[\n",
        "    (\"Title\", \"@titles\"),\n",
        "    (\"Director(s)\", \"@directors\"),\n",
        "    (\"Cast\", \"@cast\"),\n",
        "    (\"Description\", \"@description\"),\n",
        "],\n",
        "                 point_policy=\"follow_mouse\")\n",
        "\n",
        "# map colors\n",
        "mapper = linear_cmap(field_name='desc', \n",
        "                     palette=Category20[20],\n",
        "                     low=min(y_labels) ,high=max(y_labels))\n",
        "\n",
        "# prepare the figure\n",
        "p = figure(plot_width=800, plot_height=800, \n",
        "           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n",
        "           title=\"Netflix Movies and Tv Shows, Clustered(Autoencoder and custem Keras Layer), Tf-idf with Plain Text\", \n",
        "           toolbar_location=\"right\")\n",
        "\n",
        "# plot\n",
        "p.scatter('x', 'y', size=5, \n",
        "          source=source,\n",
        "          fill_color=mapper,\n",
        "          line_alpha=0.3,\n",
        "          line_color=\"black\",\n",
        "          legend = 'labels')\n",
        "\n",
        "# option\n",
        "option = RadioButtonGroup(labels=[\"C-0\", \"C-1\", \"C-2\",\n",
        "                                  \"C-3\", \"C-4\", \"C-5\",\n",
        "                                  \"C-6\", \"C-7\", \"C-8\",\n",
        "                                  \"C-9\", \"C-10\", \"C-11\",\n",
        "                                  \"C-12\", \"C-13\", \"C-14\",\n",
        "                                  \"C-15\", \"C-16\", \"C-17\",\n",
        "                                  \"C-18\", \"C-19\", \"All\"], \n",
        "                          active=20)\n",
        "\n",
        "# search box\n",
        "#keyword = TextInput(title=\"Search:\", callback=keyword_callback)\n",
        "#header\n",
        "header = Div(text=\"\"\"<h1>Find similar movies / tv shows in corresponding Cluster</h1>\"\"\")\n",
        "\n",
        "# show\n",
        "show(column(header,p))\n"
      ],
      "metadata": {
        "id": "KGXw2k33s5sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0SZSS9DcuRxJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}